{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ticl.evaluation.baselines import tabular_baselines\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)  # openml deprecation of array return type\n",
    "from ticl.datasets import load_openml_list, open_cc_valid_dids, open_cc_dids\n",
    "from ticl.evaluation.baselines.tabular_baselines import knn_metric, catboost_metric, logistic_metric, xgb_metric, random_forest_metric, mlp_metric, hyperfast_metric, hyperfast_metric_tuning, resnet_metric, mothernet_init_metric\n",
    "from ticl.evaluation.tabular_evaluation import evaluate, eval_on_datasets, transformer_metric\n",
    "from ticl.evaluation import tabular_metrics\n",
    "from ticl.prediction.tabpfn import TabPFNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 30\n"
     ]
    }
   ],
   "source": [
    "cc_test_datasets_multiclass, cc_test_datasets_multiclass_df = load_openml_list(open_cc_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 10000, num_feats=100, return_capped=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "eval_positions = [1000]\n",
    "max_features = 100\n",
    "n_samples = 2000\n",
    "base_path = os.path.join('../')\n",
    "overwrite = False\n",
    "max_times = [1, 15, 30, 60, 60 * 5, 60 * 15, 60*60]\n",
    "metric_used = tabular_metrics.auc_metric\n",
    "task_type = 'multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ticl.evaluation.baselines.distill_mlp import DistilledTabPFNMLP\n",
    "from ticl.prediction.mothernet import MotherNetClassifier\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Evaluation\n",
    "This section runs baselines and saves results locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {base_path}/results\n",
    "!mkdir -p {base_path}/results/tabular/\n",
    "!mkdir -p {base_path}/results/tabular/multiclass/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc_test_datasets_multiclass_df['isNumeric'] = (cc_test_datasets_multiclass_df.NumberOfSymbolicFeatures == 1) & (cc_test_datasets_multiclass_df.NumberOfInstancesWithMissingValues == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlrrr}\n",
      "\\toprule\n",
      "did & name & d & n & k \\\\\n",
      "\\midrule\n",
      "11 & balance-scale & 5 & 625 & 3 \\\\\n",
      "14 & mfeat-fourier & 77 & 2000 & 10 \\\\\n",
      "15 & breast-w & 10 & 699 & 2 \\\\\n",
      "16 & mfeat-karhunen & 65 & 2000 & 10 \\\\\n",
      "18 & mfeat-morphological & 7 & 2000 & 10 \\\\\n",
      "22 & mfeat-zernike & 48 & 2000 & 10 \\\\\n",
      "23 & cmc & 10 & 1473 & 3 \\\\\n",
      "29 & credit-approval & 16 & 690 & 2 \\\\\n",
      "31 & credit-g & 21 & 1000 & 2 \\\\\n",
      "37 & diabetes & 9 & 768 & 2 \\\\\n",
      "50 & tic-tac-toe & 10 & 958 & 2 \\\\\n",
      "54 & vehicle & 19 & 846 & 4 \\\\\n",
      "188 & eucalyptus & 20 & 736 & 5 \\\\\n",
      "458 & analcatdata_authorship & 71 & 841 & 4 \\\\\n",
      "469 & analcatdata_dmft & 5 & 797 & 6 \\\\\n",
      "1049 & pc4 & 38 & 1458 & 2 \\\\\n",
      "1050 & pc3 & 38 & 1563 & 2 \\\\\n",
      "1063 & kc2 & 22 & 522 & 2 \\\\\n",
      "1068 & pc1 & 22 & 1109 & 2 \\\\\n",
      "1462 & banknote-authentication & 5 & 1372 & 2 \\\\\n",
      "1464 & blood-transfusion-service-center & 5 & 748 & 2 \\\\\n",
      "1480 & ilpd & 11 & 583 & 2 \\\\\n",
      "1494 & qsar-biodeg & 42 & 1055 & 2 \\\\\n",
      "1510 & wdbc & 31 & 569 & 2 \\\\\n",
      "6332 & cylinder-bands & 40 & 540 & 2 \\\\\n",
      "23381 & dresses-sales & 13 & 500 & 2 \\\\\n",
      "40966 & MiceProtein & 82 & 1080 & 8 \\\\\n",
      "40975 & car & 7 & 1728 & 4 \\\\\n",
      "40982 & steel-plates-fault & 28 & 1941 & 7 \\\\\n",
      "40994 & climate-model-simulation-crashes & 21 & 540 & 2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_test_datasets_multiclass_df['NumberOfInstances'] =  cc_test_datasets_multiclass_df['NumberOfInstances'].astype(int)\n",
    "cc_test_datasets_multiclass_df['NumberOfFeatures'] =  cc_test_datasets_multiclass_df['NumberOfFeatures'].astype(int)\n",
    "cc_test_datasets_multiclass_df['NumberOfClasses'] =  cc_test_datasets_multiclass_df['NumberOfClasses'].astype(int)\n",
    "\n",
    "print(cc_test_datasets_multiclass_df[['did', 'name', 'NumberOfFeatures', 'NumberOfInstances', 'NumberOfClasses']].rename(columns={'NumberOfFeatures': \"d\", \"NumberOfInstances\":\"n\", \"NumberOfClasses\": \"k\"}).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overlap:\n",
    "balance-scale\n",
    "mfeat-fourier\n",
    "mfeat-karhunen\n",
    "mfeat-morphological\n",
    "credit-g\n",
    "tic-tac-toe\n",
    "vehicle\n",
    "analcatdata_authorship\n",
    "analcatdata_dmft\n",
    "pc3\n",
    "pc1\n",
    "blood-transfusion-service-center\n",
    "ilpd\n",
    "qsar-biodeg\n",
    "MiceProtein\n",
    "car\n",
    "steel-plates-fault\n",
    "climate-simulation-model-crashes\n",
    "\n",
    "non-overlap:\n",
    "breast-w (valid)\n",
    "mfeat-zernike (valid)\n",
    "mcm (valid)\n",
    "eucalyptus (valid)\n",
    "wdbc (valid)\n",
    "cylinder-bands (valid)\n",
    "dresses-sales (valid)\n",
    "\n",
    "\n",
    "banknote-authentication (test)\n",
    "credit-approval (test)\n",
    "diabetes (test)\n",
    "pc4 (test)\n",
    "kc2 (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating knn on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 525 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating rf_new_params on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 859 out of 1050 | elapsed:    0.6s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating xgb on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 859 out of 1050 | elapsed:    0.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating logistic on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mlp on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from ticl.evaluation.tabular_evaluation import eval_on_datasets\n",
    "max_times = [1, 5, 15, 60, 5 * 60, 15 * 60, 60* 60]\n",
    "\n",
    "clf_dict= {\n",
    "    'knn': knn_metric,\n",
    "    'rf_new_params': random_forest_metric,\n",
    "    'xgb': xgb_metric,\n",
    "    'logistic': logistic_metric,\n",
    "    'mlp': mlp_metric}\n",
    "\n",
    "results_baselines = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, fetch_only=True)\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating hyperfast_tuning_gpu on cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 122.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from ticl.evaluation.tabular_evaluation import eval_on_datasets\n",
    "\n",
    "max_times = [60 * 60]\n",
    "clf_dict= {\n",
    "    'hyperfast_tuning_gpu': hyperfast_metric_tuning}\n",
    "results_hyperfast = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, fetch_only=False, device='cuda:2')\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mothernet_gd_gpu4 on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 123.10it/s]\n"
     ]
    }
   ],
   "source": [
    "max_times = [60 * 60]\n",
    "clf_dict= {\n",
    "    'mothernet_gd_gpu4': mothernet_init_metric}\n",
    "\n",
    "results_mothernet_gd = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, n_jobs=1, device=\"cuda:0\", verbose=0)\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mlp_gpu2 on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 115.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# MLP GPU\n",
    "max_times = [60 * 60]\n",
    "# these will all be evaluated on CPU because they are given as callables, which is a weird way to do it.\n",
    "clf_dict= {\n",
    "    'mlp_gpu2': mlp_metric}\n",
    "\n",
    "results_mlp = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, n_jobs=1, device=\"cuda:0\", verbose=0)\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating resnet_gpu on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 115.21it/s]\n"
     ]
    }
   ],
   "source": [
    "max_times = [60 * 60]\n",
    "# these will all be evaluated on CPU because they are given as callables, which is a weird way to do it.\n",
    "clf_dict= {\n",
    "    'resnet_gpu': resnet_metric}\n",
    "\n",
    "results_resnet = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, n_jobs=1, device=\"cuda:0\", verbose=0)\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mlp_distill on cpu\n",
      "evaluating tabpfn on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mothernet_init_gd_no_learning_preprocess2 on cpu\n",
      "evaluating hyperfast_no_optimize_gpu on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating hyperfast_defaults_gpu on cpu\n",
      "evaluating mothernet_ohe_ensemble_8_quantile_gpu_timing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mothernet_ohe_ensemble_8_gpu_timing_sklearn_refactor on cpu\n",
      "evaluating mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2762: UserWarning: n_quantiles (1000) is greater than the total number of samples (270). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2762: UserWarning: n_quantiles (1000) is greater than the total number of samples (270). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing_sklearn_refactor2 on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [13 19] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/anaconda/envs/tabpfn_testing_environment/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing on cpu\n",
      "evaluating mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing_sklearn_refactor on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.7s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 96 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing_again on cpu\n",
      "evaluating mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_cpu_timing3 on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  35 out of 150 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 150 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from ticl.evaluation.tabular_evaluation import eval_on_datasets\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from ticl.prediction.mothernet import ShiftClassifier, EnsembleMeta, MotherNetClassifier, MotherNetInitMLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from ticl.prediction.mothernet_additive import MotherNetAdditiveClassifier\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "\n",
    "from hyperfast import HyperFastClassifier\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(skip_parameter_validation=True, assume_finite=True)\n",
    "\n",
    "\n",
    "# transformers don't have max times\n",
    "import warnings\n",
    "max_times = [1]\n",
    "#device = \"cuda:1\"\n",
    "device = \"cpu\"\n",
    "\n",
    "ebm_default = ExplainableBoostingClassifier()\n",
    "ebm_bins = ExplainableBoostingClassifier(max_bins=64)\n",
    "ebm_bins_main_effects = ExplainableBoostingClassifier(max_bins=64, interactions=0)\n",
    "\n",
    "\n",
    "model_string = \"tabpfn_nooptimizer_emsize_512_nlayers_12_steps_2048_bs_32ada_lr_0.0001_1_gpu_07_24_2023_01_43_33\"\n",
    "tabpfn_ours = TabPFNClassifier(device=device, model_string=model_string, epoch=\"1650\", N_ensemble_configurations=3)\n",
    "mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8 = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_2910.cpkt\", device=device), n_estimators=8, onehot=True)\n",
    "mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_quantile = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_2910.cpkt\", device=device, inference_device=device), n_estimators=8, onehot=True, power=\"quantile\")\n",
    "mn_Dclass_average_03_25_2024_17_14_32_epoch_3970_ohe_ensemble_8_quantile = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_3970.cpkt\", device=device), n_estimators=8, onehot=True, power=\"quantile\")\n",
    "\n",
    "\n",
    "mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_2910.cpkt\", device=\"cpu\"), n_estimators=8, onehot=True)\n",
    "\n",
    "mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_3 = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_2910.cpkt\", device=device), n_estimators=3, onehot=True)\n",
    "\n",
    "baam_nsamples500_numfeatures10_04_07_2024_17_04_53_epoch_1780 = MotherNetAdditiveClassifier(path=\"../models_diff/baam_nsamples500_numfeatures10_04_07_2024_17_04_53_epoch_1780.cpkt\", device=device)\n",
    "\n",
    "mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_categoricalfeaturep0.9_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270.cpkt\", device=device, inference_device=device), n_estimators=8, onehot=True, power=\"quantile\")\n",
    "mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_cpu_timing = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_categoricalfeaturep0.9_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270.cpkt\", device=\"cpu\"), n_estimators=8, onehot=True, power=\"quantile\")\n",
    "\n",
    "\n",
    "mlp_distill = make_pipeline(StandardScaler(), DistilledTabPFNMLP(n_epochs=1000, device=device, hidden_size=128, n_layers=2, dropout_rate=.1, learning_rate=0.01, model_string=model_string, epoch=1650, N_ensemble_configurations=3))\n",
    "mothernet_21_46_25_3940_ensemble3 = EnsembleMeta(MotherNetClassifier(path=\"../models_diff/mn_d2048_H4096_L2_W32_P512_1_gpu_warm_08_25_2023_21_46_25_epoch_3940_no_optimizer.pickle\", device=device), n_estimators=3)\n",
    "ebm_bins_main_effects = ExplainableBoostingClassifier(max_bins=64, interactions=0)\n",
    "baam_nfeatures_20_no_ensemble_e1520 = MotherNetAdditiveClassifier(path=\"../models_diff/baam_H512_Dclass_average_e128_nsamples500_numfeatures20_padzerosFalse_03_14_2024_15_03_22_epoch_1520.cpkt\", device=device)\n",
    "mothernet_init_gd_no_learning = MotherNetInitMLPClassifier(path=\"../models_diff/mn_Dclass_average_03_25_2024_17_14_32_epoch_2910.cpkt\", device=device, n_epochs=1, learning_rate=0.0)\n",
    "\n",
    "\n",
    "clf_dict= {\n",
    "    #'mothernet': partial(transformer_metric, classifier=mothernet_21_46_25_3940_ensemble3, onehot=True),\n",
    "    'mlp_distill': mlp_distill,\n",
    "    'tabpfn': transformer_metric,\n",
    "    #'mothernet_init_gd_no_learning': mothernet_init_gd_no_learning,\n",
    "    'mothernet_init_gd_no_learning_preprocess2': partial(transformer_metric, classifier=mothernet_init_gd_no_learning, onehot=True),\n",
    "    #'tabpfn_ours': tabpfn_ours,\n",
    "    \"hyperfast_no_optimize_gpu\": partial(hyperfast_metric, optimization=None),\n",
    "    \"hyperfast_defaults_gpu\": hyperfast_metric,\n",
    "    #'ebm_default': partial(transformer_metric, classifier=ebm_default),\n",
    "    #'ebm_bins': partial(transformer_metric, classifier=ebm_bins),\n",
    "    #'ebm_bins_main_effects': partial(transformer_metric, classifier=ebm_bins_main_effects),\n",
    "    #'baam_nfeatures_20_no_ensemble_e1520': baam_nfeatures_20_no_ensemble_e1520,\n",
    "    #'mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8,\n",
    "    #'mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_3': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_3,\n",
    "    #'baam_nsamples500_numfeatures10_04_07_2024_17_04_53_epoch_1780': baam_nsamples500_numfeatures10_04_07_2024_17_04_53_epoch_1780,\n",
    "\n",
    "     'mothernet_ohe_ensemble_8_quantile_gpu_timing': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_quantile,\n",
    "    #'mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_quantile': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_quantile,\n",
    "    'mothernet_ohe_ensemble_8_gpu_timing_sklearn_refactor': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8,\n",
    "\n",
    "    #'mn_Dclass_average_03_25_2024_17_14_32_epoch_3970_ohe_ensemble_8_quantile': mn_Dclass_average_03_25_2024_17_14_32_epoch_3970_ohe_ensemble_8_quantile,\n",
    "    \n",
    "    'mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing,\n",
    "        'mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing_sklearn_refactor2': mn_Dclass_average_03_25_2024_17_14_32_epoch_2910_ohe_ensemble_8_cpu_timing,\n",
    "\n",
    "    'mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing': mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing,\n",
    "    'mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing_sklearn_refactor': mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing,\n",
    "    'mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing_again': mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_gpu_timing,\n",
    "\n",
    "    \n",
    "    'mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_cpu_timing3': mn_categoricalfeaturep09_numfeaturessamplerdouble_sample_05_09_2024_23_39_30_epoch_1270_ohe_quantile_8_cpu_timing,\n",
    "\n",
    "\n",
    "    }\n",
    "results_transformers = [\n",
    "    eval_on_datasets('multiclass', model, model_name, cc_test_datasets_multiclass, eval_positions=eval_positions, max_times=max_times,\n",
    "                     metric_used=metric_used, split_numbers=[1, 2, 3, 4, 5],\n",
    "                     n_samples=n_samples, base_path=base_path, overwrite=False, n_jobs=-1, device=device)\n",
    "    for model_name, model in clf_dict.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flat_results = []\n",
    "for per_dataset in results_baselines + results_transformers + results_hyperfast + results_mlp + results_resnet + results_mothernet_gd:\n",
    "    for result in per_dataset:\n",
    "        row = {}\n",
    "        for key in ['dataset', 'model', 'mean_metric', 'split', 'max_time']:\n",
    "            row[key] = result[key]\n",
    "        best_configs_key, = [k for k in result.keys() if \"best_configs\" in k]\n",
    "        if result[best_configs_key][0] is not None:\n",
    "            row.update(result[best_configs_key][0])\n",
    "        row['mean_metric'] = float(row[\"mean_metric\"].numpy())\n",
    "        flat_results.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(flat_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>mean_metric</th>\n",
       "      <th>split</th>\n",
       "      <th>max_time</th>\n",
       "      <th>best</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>num_trials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 14}</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.031152</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.848925</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 8}</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.026842</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.852651</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.027301</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.885874</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>balance-scale</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.895205</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 15}</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7795</th>\n",
       "      <td>climate-model-simulation-crashes</td>\n",
       "      <td>mothernet_gd_gpu4</td>\n",
       "      <td>0.926609</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>{'dropout_rate': 0.3, 'learning_rate': 0.00057...</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>climate-model-simulation-crashes</td>\n",
       "      <td>mothernet_gd_gpu4</td>\n",
       "      <td>0.937867</td>\n",
       "      <td>2</td>\n",
       "      <td>3600</td>\n",
       "      <td>{'dropout_rate': 0.1, 'learning_rate': 1.01205...</td>\n",
       "      <td>0.256001</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7797</th>\n",
       "      <td>climate-model-simulation-crashes</td>\n",
       "      <td>mothernet_gd_gpu4</td>\n",
       "      <td>0.935806</td>\n",
       "      <td>3</td>\n",
       "      <td>3600</td>\n",
       "      <td>{'dropout_rate': 0.1, 'learning_rate': 7.49013...</td>\n",
       "      <td>0.267680</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7798</th>\n",
       "      <td>climate-model-simulation-crashes</td>\n",
       "      <td>mothernet_gd_gpu4</td>\n",
       "      <td>0.937669</td>\n",
       "      <td>4</td>\n",
       "      <td>3600</td>\n",
       "      <td>{'dropout_rate': 0.1, 'learning_rate': 2.63955...</td>\n",
       "      <td>0.247117</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7799</th>\n",
       "      <td>climate-model-simulation-crashes</td>\n",
       "      <td>mothernet_gd_gpu4</td>\n",
       "      <td>0.934567</td>\n",
       "      <td>5</td>\n",
       "      <td>3600</td>\n",
       "      <td>{'dropout_rate': 0.3, 'learning_rate': 1.23130...</td>\n",
       "      <td>0.238844</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>909.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7800 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               dataset              model  mean_metric  split  \\\n",
       "0                        balance-scale                knn     0.898451      1   \n",
       "1                        balance-scale                knn     0.848925      2   \n",
       "2                        balance-scale                knn     0.852651      3   \n",
       "3                        balance-scale                knn     0.885874      4   \n",
       "4                        balance-scale                knn     0.895205      5   \n",
       "...                                ...                ...          ...    ...   \n",
       "7795  climate-model-simulation-crashes  mothernet_gd_gpu4     0.926609      1   \n",
       "7796  climate-model-simulation-crashes  mothernet_gd_gpu4     0.937867      2   \n",
       "7797  climate-model-simulation-crashes  mothernet_gd_gpu4     0.935806      3   \n",
       "7798  climate-model-simulation-crashes  mothernet_gd_gpu4     0.937669      4   \n",
       "7799  climate-model-simulation-crashes  mothernet_gd_gpu4     0.934567      5   \n",
       "\n",
       "      max_time                                               best  fit_time  \\\n",
       "0            1                                {'n_neighbors': 14}  0.000790   \n",
       "1            1                                 {'n_neighbors': 8}  0.000795   \n",
       "2            1                                {'n_neighbors': 10}  0.000851   \n",
       "3            1                                {'n_neighbors': 10}  0.000786   \n",
       "4            1                                {'n_neighbors': 15}  0.000829   \n",
       "...        ...                                                ...       ...   \n",
       "7795      3600  {'dropout_rate': 0.3, 'learning_rate': 0.00057...  0.051890   \n",
       "7796      3600  {'dropout_rate': 0.1, 'learning_rate': 1.01205...  0.256001   \n",
       "7797      3600  {'dropout_rate': 0.1, 'learning_rate': 7.49013...  0.267680   \n",
       "7798      3600  {'dropout_rate': 0.1, 'learning_rate': 2.63955...  0.247117   \n",
       "7799      3600  {'dropout_rate': 0.3, 'learning_rate': 1.23130...  0.238844   \n",
       "\n",
       "      inference_time  num_trials  \n",
       "0           0.031152         NaN  \n",
       "1           0.026842         NaN  \n",
       "2           0.027301         NaN  \n",
       "3           0.028868         NaN  \n",
       "4           0.027410         NaN  \n",
       "...              ...         ...  \n",
       "7795        0.000955       903.0  \n",
       "7796        0.000964       902.0  \n",
       "7797        0.001275       937.0  \n",
       "7798        0.000973       934.0  \n",
       "7799        0.000834       909.0  \n",
       "\n",
       "[7800 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(\"results_test.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(results_baselines + results_transformers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df['model'] = results_df.model.replace({'knn': \"KNN\", 'rf_new_params': 'RF', 'mlp': \"MLP\",'mlp_distill': 'MLP-Distill', 'xgb':'XGBoost', 'logistic': 'LogReg',  'mothernet': 'MotherNet', 'tabpfn': 'TabPFN (Hollmann)', 'tabpfn_ours': 'TabPFN (ours)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/results_test_for_mothernet_paper_2024-05-21.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "filename = f\"results/results_test_for_mothernet_paper_{datetime.date.today()}.csv\"\n",
    "results_df.to_csv(filename)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mothernet]",
   "language": "python",
   "name": "conda-env-mothernet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
