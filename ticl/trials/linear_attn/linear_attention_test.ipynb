{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, sys\n",
    "import numpy as np\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "import inspect\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import pandas as pd\n",
    "from fast_transformers.builders import TransformerEncoderBuilder\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from fast_transformers.masking import FullMask, LengthMask\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(root_dir)\n",
    "from benchmark import get_csv, benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "num_heads = 2\n",
    "embed_dim = 32\n",
    "n_hid = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of parameters\n",
    "def get_num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Create the builder for our transformers\n",
    "builder = TransformerEncoderBuilder.from_kwargs(\n",
    "    n_layers=n_layers,\n",
    "    n_heads=num_heads,\n",
    "    query_dimensions=embed_dim // num_heads,\n",
    "    value_dimensions=embed_dim // num_heads,\n",
    "    feed_forward_dimensions=n_hid\n",
    ")\n",
    "\n",
    "# Build a transformer with softmax attention\n",
    "builder.attention_type = \"full\"\n",
    "softmax_model = builder.get().to('cuda')\n",
    "\n",
    "# Build a transformer with linear attention\n",
    "builder.attention_type = \"linear\"\n",
    "linear_model = builder.get().to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(\n",
    "    TransformerEncoderLayer(\n",
    "        d_model = embed_dim,\n",
    "        nhead = num_heads,\n",
    "        dim_feedforward = n_hid,\n",
    "        batch_first = True,\n",
    "    ),\n",
    "    num_layers = n_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Layer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size\n",
      "| Standard Attention |  Their Implementation  |:  15202\n",
      "|  Linear Attention  |  Their Implementation  |:  15202\n",
      "| Standard Attention | Pytorch Implementation |:  15138\n"
     ]
    }
   ],
   "source": [
    "print('Model Size')\n",
    "print(\"| Standard Attention |  Their Implementation  |: \", get_num_params(softmax_model))\n",
    "print(\"|  Linear Attention  |  Their Implementation  |: \", get_num_params(linear_model))\n",
    "print(\"| Standard Attention | Pytorch Implementation |: \", get_num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time\n",
      "| Standard Attention | Pytorch Implementation |:  36.94 (0.00) ms\n",
      "| Standard Attention |  Their Implementation  |:  nan (nan) ms\n",
      "|  Linear Attention  |  Their Implementation  |:  1.82 (0.00) ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/linear_attention/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/anaconda/envs/linear_attention/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda/envs/linear_attention/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/anaconda/envs/linear_attention/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/anaconda/envs/linear_attention/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print('Inference Time')\n",
    "\n",
    "batch_size = 10\n",
    "seq_len = int(1e3)\n",
    "num_simulations = 1\n",
    "\n",
    "time_elaspsed = {'softmax':[], 'linear':[], 'pytorch':[]}\n",
    "for _ in range(num_simulations):\n",
    "    \n",
    "    # Construct the dummy input\n",
    "    X = torch.rand(batch_size, seq_len, embed_dim)\n",
    "\n",
    "    # Prepare everythin for CUDA\n",
    "    X = X.cuda()\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start.record()\n",
    "        y = model(X)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        time_elaspsed['pytorch'].append(start.elapsed_time(end))\n",
    "\n",
    "print(\"| Standard Attention | Pytorch Implementation |: \", f\"{np.mean(time_elaspsed['pytorch']):.2f}\", f\"({np.std(time_elaspsed['pytorch']):.2f})\", \"ms\")\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "\n",
    "    softmax_model.cuda()\n",
    "    softmax_model.eval()\n",
    "    linear_model.cuda()\n",
    "    linear_model.eval()\n",
    "\n",
    "    # Warmup the GPU\n",
    "    with torch.no_grad():\n",
    "        # softmax_model(X)\n",
    "        linear_model(X)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Measure the execution time\n",
    "    softmax_start = torch.cuda.Event(enable_timing=True)\n",
    "    softmax_end = torch.cuda.Event(enable_timing=True)\n",
    "    linear_start = torch.cuda.Event(enable_timing=True)\n",
    "    linear_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     softmax_start.record()\n",
    "    #     y = softmax_model(X)\n",
    "    #     softmax_end.record()\n",
    "    #     torch.cuda.synchronize()\n",
    "    #     time_elaspsed['softmax'].append(softmax_start.elapsed_time(softmax_end))\n",
    "        \n",
    "    #     # Softmax: 144 ms (on a GTX1080Ti)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        linear_start.record()\n",
    "        y = linear_model(X)\n",
    "        linear_end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        time_elaspsed['linear'].append(linear_start.elapsed_time(linear_end))\n",
    "        \n",
    "        # Linear: 68 ms (on a GTX1080Ti)\n",
    "\n",
    "print(\"| Standard Attention |  Their Implementation  |: \", f\"{np.mean(time_elaspsed['softmax']):.2f}\", f\"({np.std(time_elaspsed['softmax']):.2f})\", \"ms\")\n",
    "print(\"|  Linear Attention  |  Their Implementation  |: \", f\"{np.mean(time_elaspsed['linear']):.2f}\", f\"({np.std(time_elaspsed['linear']):.2f})\", \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_attention(q = None, k = None, v = None, is_causal = False, return_model = False, **kwargs):\n",
    "    \n",
    "    # q, k, v: [batch_size*num_heads, seq_len, embed_dim]\n",
    "    if is_causal: raise NotImplementedError(\"Causal attention is not implemented for linear attention\")\n",
    "    \n",
    "    linear_attn = linear_model.layers[0].attention\n",
    "    if return_model: return linear_attn\n",
    "    \n",
    "    return linear_attn(\n",
    "        q,k,v,\n",
    "        attn_mask=None,\n",
    "        query_lengths=None,\n",
    "        key_lengths=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_tf(x, src_mask = None, is_causal = False):\n",
    "    if is_causal: raise NotImplementedError(\"Causal attention is not implemented for linear transformer\") \n",
    "    if src_mask: raise NotImplementedError(\"Masking is not implemented for linear transformer\")\n",
    "    \n",
    "    linear_transformer_layer = linear_model.layers[0]\n",
    "\n",
    "    if src_mask is None:\n",
    "        return linear_transformer_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, **kwargs):\n",
    "    if model_name == 'flash_attention':\n",
    "        model = fa\n",
    "    elif model_name == 'flash_linear_attention':\n",
    "        model = fla\n",
    "    elif model_name == 'flash_linear_tf':\n",
    "        model = fla_tf\n",
    "    elif model_name == 'flash_tf':\n",
    "        model = fa_tf\n",
    "    elif model_name == 'linear_attention':\n",
    "        model = linear_attention\n",
    "    elif model_name == 'linear_tf':\n",
    "        model = linear_tf\n",
    "    elif model_name == 'simplified_linear_attention':\n",
    "        from linear_attn_forward import linear_attention as simplified_linear_attention\n",
    "        model = simplified_linear_attention(default_mask = True, event_dispatcher = True, **kwargs).to('cuda')\n",
    "    else:\n",
    "        raise ValueError(f\"model_name {model_name} not supported\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention\n",
    "benchmark('linear_attention', get_model, is_causal = False, self_attn = True, overwrite = overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention\n",
    "benchmark('simplified_linear_attention', get_model, is_causal = False, self_attn = True, overwrite = overwrite, max_len_power = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark('linear_tf', get_model, is_causal = False, self_attn = True, overwrite = overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip head dim 32, 2 heads, length-16.\n",
      "Skip head dim 32, 2 heads, length-32.\n",
      "Skip head dim 32, 2 heads, length-64.\n",
      "Skip head dim 32, 2 heads, length-128.\n",
      "Skip head dim 32, 2 heads, length-256.\n",
      "Skip head dim 32, 2 heads, length-512.\n",
      "Skip head dim 32, 2 heads, length-1024.\n",
      "Skip head dim 32, 2 heads, length-2048.\n",
      "Skip head dim 32, 2 heads, length-4096.\n",
      "Skip head dim 32, 2 heads, length-8192.\n",
      "Skip head dim 32, 2 heads, length-16384.\n",
      "Skip head dim 32, 2 heads, length-32768.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerEncoderLayer.forward() got an unexpected keyword argument 'src_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cross-attention\u001b[39;00m\n\u001b[1;32m      2\u001b[0m benchmark(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_attention\u001b[39m\u001b[38;5;124m'\u001b[39m, is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, self_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, overwrite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear_tf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(model_name, is_causal, n_repeat, batch_size, seed, self_attn, overwrite)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_repeat):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m         start \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mEvent(enable_timing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mbenchmark.<locals>.run_model\u001b[0;34m(model, q, k, v, is_causal, self_attn)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate([k, q], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     model(q, k, v)\n",
      "File \u001b[0;32m/anaconda/envs/linear_attention/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerEncoderLayer.forward() got an unexpected keyword argument 'src_mask'"
     ]
    }
   ],
   "source": [
    "# cross-attention\n",
    "benchmark('linear_attention', get_model, is_causal = False, self_attn = False, overwrite = False)\n",
    "benchmark('linear_tf', get_model, is_causal = False, self_attn = False, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def dispatch(self, event):\n",
      "        \"\"\"Dispatch an event to the listeners.\n",
      "\n",
      "        Arguments\n",
      "        ---------\n",
      "            event: Event instance\n",
      "        \"\"\"\n",
      "        for event_handler, event_filter in self._listeners.items():\n",
      "            if event_filter(event):\n",
      "                event_handler(event)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(linear_model.layers[0].attention.event_dispatcher.dispatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_dispatchers',\n",
       " '_listeners',\n",
       " 'clear',\n",
       " 'dispatch',\n",
       " 'get',\n",
       " 'listen',\n",
       " 'remove']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the function of a class\n",
    "\n",
    "dir(linear_model.layers[0].attention.event_dispatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, queries, keys, values, attn_mask, query_lengths,\n",
      "                key_lengths):\n",
      "        # Apply the feature map to the queries and keys\n",
      "        self.feature_map.new_feature_map(queries.device)\n",
      "        Q = self.feature_map.forward_queries(queries)\n",
      "        K = self.feature_map.forward_keys(keys)\n",
      "\n",
      "        # Apply the key padding mask and make sure that the attn_mask is\n",
      "        # all_ones\n",
      "        if not attn_mask.all_ones:\n",
      "            raise RuntimeError((\"LinearAttention does not support arbitrary \"\n",
      "                                \"attention masks\"))\n",
      "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
      "\n",
      "        # Compute the KV matrix, namely the dot product of keys and values so\n",
      "        # that we never explicitly compute the attention matrix and thus\n",
      "        # decrease the complexity\n",
      "        KV = torch.einsum(\"nshd,nshm->nhmd\", K, values)\n",
      "\n",
      "        # Compute the normalizer\n",
      "        Z = 1/(torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1))+self.eps)\n",
      "\n",
      "        # Finally compute and return the new values\n",
      "        V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", Q, KV, Z)\n",
      "\n",
      "        return V.contiguous()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(linear_model.layers[0].attention.inner_attention.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, queries, keys, values, attn_mask, query_lengths,\n",
      "                key_lengths):\n",
      "        # Apply the feature map to the queries and keys\n",
      "        self.feature_map.new_feature_map(queries.device)\n",
      "        Q = self.feature_map.forward_queries(queries)\n",
      "        K = self.feature_map.forward_keys(keys)\n",
      "\n",
      "        # Apply the key padding mask and make sure that the attn_mask is\n",
      "        # all_ones\n",
      "        if not attn_mask.all_ones:\n",
      "            raise RuntimeError((\"LinearAttention does not support arbitrary \"\n",
      "                                \"attention masks\"))\n",
      "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
      "\n",
      "        # Compute the KV matrix, namely the dot product of keys and values so\n",
      "        # that we never explicitly compute the attention matrix and thus\n",
      "        # decrease the complexity\n",
      "        KV = torch.einsum(\"nshd,nshm->nhmd\", K, values)\n",
      "\n",
      "        # Compute the normalizer\n",
      "        Z = 1/(torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1))+self.eps)\n",
      "\n",
      "        # Finally compute and return the new values\n",
      "        V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", Q, KV, Z)\n",
      "\n",
      "        return V.contiguous()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(linear_model.layers[0].attention.inner_attention.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, model_name, q, k, v, is_causal, self_attn = False):\n",
    "    if 'tf' in model_name:\n",
    "        if self_attn: \n",
    "            return model(q, is_causal = is_causal)\n",
    "        else:\n",
    "            x = torch.concatenate([k, q], dim = 1)\n",
    "            return model(x, src_mask = k.shape[0], is_causal = is_causal)\n",
    "    else:\n",
    "        return model(q, k, v, is_causal = is_causal, need_weights = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "q = torch.randn(4, int(1e5), embed_dim, device = \"cuda\")\n",
    "k = q\n",
    "v = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero 948 MB\n",
      "1 948 MB\n",
      "2 10485 MB\n",
      "begin 20022 MB\n",
      "init 20022 MB\n",
      "146 MB\n",
      "98 MB\n",
      "301 MB\n",
      "301 MB\n",
      "19667.7890625 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = 'linear_attention'\n",
    "model1 = get_model(model_name, d_model = embed_dim, n_heads = num_heads)\n",
    "memory_before = torch.cuda.memory_allocated(device=\"cuda\")\n",
    "torch.cuda.reset_peak_memory_stats(device='cuda')\n",
    "out1 = run_model(model1, model_name, q, k, v, is_causal = False, self_attn = True)\n",
    "memory_after = torch.cuda.max_memory_allocated(device=\"cuda\")\n",
    "print((memory_after - memory_before)/1024/1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init 1943 MB\n",
      "146 MB\n",
      "98 MB\n",
      "301 MB\n",
      "301 MB\n",
      "593.95849609375 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = 'simplified_linear_attention'\n",
    "model2 = get_model(model_name, d_model = embed_dim, n_heads = num_heads)\n",
    "memory_before = torch.cuda.memory_allocated(device=\"cuda\")\n",
    "torch.cuda.reset_peak_memory_stats(device='cuda')\n",
    "out2 = run_model(model2, model_name, q, k, v, is_causal = False, self_attn = True)\n",
    "memory_after = torch.cuda.max_memory_allocated(device=\"cuda\")\n",
    "print((memory_after - memory_before)/1024/1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero 1943 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4224, 4224)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(model1(return_model=True)), get_num_params(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1336, -0.0488,  0.0153,  ..., -0.0899, -0.0093,  0.0702],\n",
       "         [ 0.1328, -0.0508,  0.0197,  ..., -0.0829, -0.0143,  0.0658],\n",
       "         [ 0.1335, -0.0461,  0.0136,  ..., -0.0863, -0.0101,  0.0698],\n",
       "         ...,\n",
       "         [ 0.1402, -0.0447,  0.0101,  ..., -0.0888, -0.0085,  0.0757],\n",
       "         [ 0.1318, -0.0488,  0.0215,  ..., -0.0851, -0.0136,  0.0669],\n",
       "         [ 0.1386, -0.0476,  0.0129,  ..., -0.0905, -0.0051,  0.0717]],\n",
       "\n",
       "        [[ 0.1363, -0.0445,  0.0179,  ..., -0.0867, -0.0094,  0.0728],\n",
       "         [ 0.1417, -0.0490,  0.0140,  ..., -0.0859, -0.0093,  0.0693],\n",
       "         [ 0.1401, -0.0488,  0.0130,  ..., -0.0826, -0.0119,  0.0694],\n",
       "         ...,\n",
       "         [ 0.1387, -0.0467,  0.0126,  ..., -0.0902, -0.0058,  0.0708],\n",
       "         [ 0.1370, -0.0471,  0.0149,  ..., -0.0863, -0.0128,  0.0706],\n",
       "         [ 0.1357, -0.0453,  0.0147,  ..., -0.0839, -0.0108,  0.0720]],\n",
       "\n",
       "        [[ 0.1377, -0.0540,  0.0188,  ..., -0.0813, -0.0166,  0.0633],\n",
       "         [ 0.1358, -0.0501,  0.0162,  ..., -0.0831, -0.0102,  0.0689],\n",
       "         [ 0.1346, -0.0485,  0.0160,  ..., -0.0862, -0.0090,  0.0652],\n",
       "         ...,\n",
       "         [ 0.1387, -0.0499,  0.0131,  ..., -0.0817, -0.0108,  0.0708],\n",
       "         [ 0.1320, -0.0470,  0.0191,  ..., -0.0854, -0.0115,  0.0628],\n",
       "         [ 0.1382, -0.0440,  0.0146,  ..., -0.0867, -0.0104,  0.0697]],\n",
       "\n",
       "        [[ 0.1446, -0.0434,  0.0099,  ..., -0.0897,  0.0007,  0.0758],\n",
       "         [ 0.1366, -0.0460,  0.0174,  ..., -0.0822, -0.0140,  0.0738],\n",
       "         [ 0.1427, -0.0445,  0.0098,  ..., -0.0867, -0.0014,  0.0729],\n",
       "         ...,\n",
       "         [ 0.1402, -0.0378,  0.0082,  ..., -0.0935, -0.0002,  0.0777],\n",
       "         [ 0.1369, -0.0448,  0.0111,  ..., -0.0884, -0.0062,  0.0744],\n",
       "         [ 0.1372, -0.0446,  0.0134,  ..., -0.0849, -0.0102,  0.0724]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0769, -0.2752,  0.0800,  ...,  0.0190,  0.1057, -0.0327],\n",
       "         [-0.0724, -0.2785,  0.0758,  ...,  0.0153,  0.1082, -0.0369],\n",
       "         [-0.0728, -0.2790,  0.0752,  ...,  0.0193,  0.1057, -0.0334],\n",
       "         ...,\n",
       "         [-0.0785, -0.2749,  0.0829,  ...,  0.0257,  0.1028, -0.0269],\n",
       "         [-0.0739, -0.2791,  0.0716,  ...,  0.0202,  0.1076, -0.0367],\n",
       "         [-0.0730, -0.2789,  0.0733,  ...,  0.0174,  0.1085, -0.0368]],\n",
       "\n",
       "        [[-0.0721, -0.2755,  0.0802,  ...,  0.0214,  0.1057, -0.0294],\n",
       "         [-0.0742, -0.2782,  0.0734,  ...,  0.0203,  0.1065, -0.0372],\n",
       "         [-0.0752, -0.2760,  0.0787,  ...,  0.0213,  0.1037, -0.0304],\n",
       "         ...,\n",
       "         [-0.0727, -0.2758,  0.0814,  ...,  0.0247,  0.1030, -0.0257],\n",
       "         [-0.0761, -0.2781,  0.0779,  ...,  0.0280,  0.1048, -0.0293],\n",
       "         [-0.0760, -0.2721,  0.0851,  ...,  0.0336,  0.1062, -0.0236]],\n",
       "\n",
       "        [[-0.0719, -0.2774,  0.0760,  ...,  0.0216,  0.1070, -0.0319],\n",
       "         [-0.0759, -0.2768,  0.0812,  ...,  0.0181,  0.1054, -0.0329],\n",
       "         [-0.0704, -0.2803,  0.0744,  ...,  0.0166,  0.1090, -0.0363],\n",
       "         ...,\n",
       "         [-0.0701, -0.2781,  0.0829,  ...,  0.0202,  0.1039, -0.0287],\n",
       "         [-0.0797, -0.2748,  0.0844,  ...,  0.0195,  0.1039, -0.0277],\n",
       "         [-0.0698, -0.2772,  0.0758,  ...,  0.0198,  0.1071, -0.0332]],\n",
       "\n",
       "        [[-0.0718, -0.2764,  0.0768,  ...,  0.0236,  0.1061, -0.0331],\n",
       "         [-0.0712, -0.2764,  0.0732,  ...,  0.0174,  0.1066, -0.0382],\n",
       "         [-0.0753, -0.2741,  0.0799,  ...,  0.0245,  0.1016, -0.0297],\n",
       "         ...,\n",
       "         [-0.0718, -0.2759,  0.0755,  ...,  0.0169,  0.1052, -0.0346],\n",
       "         [-0.0710, -0.2801,  0.0733,  ...,  0.0209,  0.1082, -0.0337],\n",
       "         [-0.0763, -0.2739,  0.0826,  ...,  0.0182,  0.1048, -0.0301]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FullMask(10, device=q.device).float_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm_clone_amueller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
